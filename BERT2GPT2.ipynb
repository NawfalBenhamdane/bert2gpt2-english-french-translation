{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##  PrÃ©paration du jeu de donnÃ©es anglaisâ€“franÃ§ais\n",
        "\n",
        "Nous utilisons le dataset public de Kaggle Language Translation (Englishâ€“French) contenant deux colonnes : phrases anglaises et leurs Ã©quivalents franÃ§ais.  \n",
        "Les donnÃ©es sont chargÃ©es, renommÃ©es, nettoyÃ©es (valeurs manquantes, doublons, longueurs extrÃªmes) puis divisÃ©es en deux sous-ensembles :  \n",
        "-Train : 90 % des exemples pour lâ€™entraÃ®nement du modÃ¨le  \n",
        "-Validation : 10 % pour lâ€™Ã©valuation des performances  \n",
        "\n",
        "AprÃ¨s nettoyage, on obtient environ 158 000 paires pour lâ€™entraÃ®nement et 17 000 pour la validation.  \n",
        "Ce dataset servira Ã  alimenter le modÃ¨le de traduction basÃ© sur lâ€™architecture BERT (encodeur) et GPT-2 (dÃ©codeur).\n"
      ],
      "metadata": {
        "id": "b1Pun9PTwXIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "RN27fD-_uRja"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(\"eng_-french.csv\")\n"
      ],
      "metadata": {
        "id": "VlzrECs7DcLd"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = df.rename(columns={\n",
        "    \"English words/sentences\": \"en\",\n",
        "    \"French words/sentences\": \"fr\"\n",
        "})\n",
        "\n",
        "df = df.dropna()\n",
        "df = df.drop_duplicates()\n",
        "df = df[(df[\"en\"].str.len() > 2) & (df[\"fr\"].str.len() > 2)]\n",
        "df = df[df[\"en\"].str.len() < 200]\n",
        "df = df[df[\"fr\"].str.len() < 200]\n",
        "\n",
        "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
        "print(f\"Train: {len(train_df)} | Val: {len(val_df)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNDBVVHZDb2m",
        "outputId": "05a97190-7539-45b8-f411-d2098b9448e2"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 158051 | Val: 17562\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Initialisation des modÃ¨les et tokenizers\n",
        "\n",
        "Dans cette partie, on importe les bibliothÃ¨ques nÃ©cessaires (**Transformers**, **PyTorch**) et on charge deux modÃ¨les prÃ©entraÃ®nÃ©s :\n",
        "\n",
        "- **Encodeur : `bert-base-uncased`**  \n",
        "  Sert Ã  comprendre les phrases anglaises et Ã  produire leurs reprÃ©sentations vectorielles (embeddings contextuels).\n",
        "\n",
        "- **DÃ©codeur : `dbddv01/gpt2-french-small`**  \n",
        "  Sert Ã  gÃ©nÃ©rer les phrases traduites en franÃ§ais Ã  partir des reprÃ©sentations fournies par lâ€™encodeur.\n",
        "\n",
        "Les **tokenizers** correspondants sont Ã©galement chargÃ©s pour convertir le texte brut en tokens numÃ©riques compatibles avec chaque modÃ¨le.  \n",
        "Lâ€™objectif est de crÃ©er une architecture **encoderâ€“decoder hybride** :  \n",
        "BERT encode la phrase anglaise â†’ GPT-2 dÃ©code et gÃ©nÃ¨re la traduction franÃ§aise.\n"
      ],
      "metadata": {
        "id": "noo0YBIGxNXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel, GPT2Model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n"
      ],
      "metadata": {
        "id": "afWi4gpYDX7Y"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_name = \"bert-base-uncased\"\n",
        "decoder_name = \"dbddv01/gpt2-french-small\"\n",
        "\n",
        "tok_en = AutoTokenizer.from_pretrained(encoder_name)\n",
        "tok_fr = AutoTokenizer.from_pretrained(decoder_name)\n",
        "\n",
        "encoder = AutoModel.from_pretrained(encoder_name)\n",
        "decoder_backbone = GPT2Model.from_pretrained(decoder_name)\n"
      ],
      "metadata": {
        "id": "b3WMQjboDZG0"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  DÃ©tails mathÃ©matiques de la Cross-Attention\n",
        "\n",
        "La **cross-attention** est le mÃ©canisme qui permet au **dÃ©codeur** (GPT-2) de \"regarder\" les reprÃ©sentations produites par lâ€™**encodeur** (BERT).  \n",
        "Elle dÃ©termine quelles parties de la phrase source influencent la gÃ©nÃ©ration de chaque mot cible.\n",
        "\n",
        "---\n",
        "\n",
        "###  1. Les entrÃ©es\n",
        "\n",
        "On dispose de trois matrices :\n",
        "\n",
        "- $Q \\in \\mathbb{R}^{T_q \\times d_k}$ : les **requÃªtes** (*queries*) venant du dÃ©codeur  \n",
        "- $K \\in \\mathbb{R}^{T_k \\times d_k}$ : les **clÃ©s** (*keys*) venant de lâ€™encodeur  \n",
        "- $V \\in \\mathbb{R}^{T_k \\times d_v}$ : les **valeurs** (*values*) venant aussi de lâ€™encodeur  \n",
        "\n",
        "Chaque vecteur reprÃ©sente le sens dâ€™un mot.  \n",
        "La cross-attention apprend Ã  relier les mots de la phrase cible (franÃ§ais) Ã  ceux de la phrase source (anglais).\n",
        "\n",
        "---\n",
        "\n",
        "###  2. Calcul des scores dâ€™attention\n",
        "\n",
        "On mesure la similaritÃ© entre chaque requÃªte et chaque clÃ© :\n",
        "\n",
        "$$\n",
        "\\text{scores} = \\frac{QK^\\top}{\\sqrt{d_k}}\n",
        "$$\n",
        "\n",
        "- Le produit $QK^\\top$ produit une matrice de taille $T_q \\times T_k$, oÃ¹ chaque Ã©lÃ©ment indique Ã  quel point un mot cible â€œregardeâ€ un mot source.  \n",
        "- Le facteur $\\frac{1}{\\sqrt{d_k}}$ Ã©vite que les valeurs soient trop grandes quand $d_k$ augmente (stabilisation numÃ©rique).\n",
        "\n",
        "---\n",
        "\n",
        "###  3. Application du softmax\n",
        "\n",
        "On transforme les scores en **poids de probabilitÃ©** :\n",
        "\n",
        "$$\n",
        "\\text{poids} = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)\n",
        "$$\n",
        "\n",
        "Chaque ligne de cette matrice correspond Ã  une distribution de probabilitÃ© :  \n",
        "les poids indiquent lâ€™importance relative de chaque mot anglais pour un mot franÃ§ais donnÃ©,  \n",
        "et la somme des poids dâ€™une ligne vaut $1$.\n",
        "\n",
        "---\n",
        "\n",
        "###  4. Combinaison avec les valeurs\n",
        "\n",
        "Les poids sont utilisÃ©s pour combiner les reprÃ©sentations $V$ de lâ€™encodeur :\n",
        "\n",
        "$$\n",
        "Z = \\text{poids} \\times V\n",
        "$$\n",
        "\n",
        "Chaque vecteur $Z_i$ est une **moyenne pondÃ©rÃ©e** des valeurs $V$,  \n",
        "oÃ¹ les poids dÃ©terminent quels mots sources contribuent le plus Ã  la prÃ©diction du mot cible.  \n",
        "\n",
        "Intuitivement :\n",
        "- le modÃ¨le \"pose une question\" via $Q$,  \n",
        "- et \"cherche la rÃ©ponse\" dans la phrase source via $K$ et $V$.\n",
        "\n",
        "---\n",
        "\n",
        "###  5. Multi-head attention\n",
        "\n",
        "En pratique, on ne fait pas une seule attention, mais plusieurs tÃªtes en parallÃ¨le :\n",
        "\n",
        "$$\n",
        "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O\n",
        "$$\n",
        "\n",
        "Chaque tÃªte apprend une maniÃ¨re diffÃ©rente de relier les mots  \n",
        "(structure grammaticale, dÃ©pendances, contexte, etc.).  \n",
        "Leur combinaison donne une vision plus complÃ¨te des relations entre mots anglais et franÃ§ais.\n",
        "\n",
        "---\n",
        "\n",
        "###  En rÃ©sumÃ©\n",
        "\n",
        "1. Lâ€™encodeur (BERT) produit des reprÃ©sentations $K$ et $V$ de la phrase anglaise.  \n",
        "2. Le dÃ©codeur (GPT-2) gÃ©nÃ¨re ses requÃªtes $Q$ pour chaque mot franÃ§ais.  \n",
        "3. La cross-attention calcule comment chaque $Q_i$ doit combiner les $V_j$ de la phrase source.  \n",
        "4. Le rÃ©sultat $Z$ guide la gÃ©nÃ©ration du mot suivant.\n",
        "\n",
        "Ainsi, la couche de **cross-attention** fait le lien mathÃ©matique entre  \n",
        "**la comprÃ©hension (encodeur)** et **la gÃ©nÃ©ration (dÃ©codeur)**.\n"
      ],
      "metadata": {
        "id": "ET1G9WILyZ9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, d_model=768, n_heads=12, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.q_proj = nn.Linear(d_model, d_model)\n",
        "        self.k_proj = nn.Linear(d_model, d_model)\n",
        "        self.v_proj = nn.Linear(d_model, d_model)\n",
        "        self.out_proj = nn.Linear(d_model, d_model)\n",
        "        self.n_heads = n_heads\n",
        "        self.d_head = d_model // n_heads\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def _split_heads(self, x):\n",
        "        B, T, D = x.shape\n",
        "        return x.view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
        "\n",
        "    def _merge_heads(self, x):\n",
        "        B, h, T, d = x.shape\n",
        "        return x.transpose(1, 2).contiguous().view(B, T, h * d)\n",
        "\n",
        "    def forward(self, q, k, v, key_padding_mask=None):\n",
        "        Q = self._split_heads(self.q_proj(q))\n",
        "        K = self._split_heads(self.k_proj(k))\n",
        "        V = self._split_heads(self.v_proj(v))\n",
        "\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_head ** 0.5)\n",
        "        if key_padding_mask is not None:\n",
        "            mask = key_padding_mask.unsqueeze(1).unsqueeze(2)\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "        context = torch.matmul(attn, V)\n",
        "        context = self._merge_heads(context)\n",
        "        return self.out_proj(context)\n"
      ],
      "metadata": {
        "id": "Q7ybRDuEDjld"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classe `EncoderDecoderNMT` â€” Architecture complÃ¨te du traducteur\n",
        "\n",
        "Cette classe assemble toutes les composantes du modÃ¨le de traduction :  \n",
        "un **encodeur BERT**, un **dÃ©codeur GPT-2**, et une **couche de cross-attention** qui relie les deux.\n",
        "\n",
        "\n",
        "### Fonctionnement du `forward`\n",
        "\n",
        "1. **Encodage**  \n",
        "   La phrase source (anglais) passe dans BERT :\n",
        "   $$\n",
        "   H_{enc} = \\text{Encoder}(x_{en})\n",
        "   $$\n",
        "\n",
        "2. **DÃ©codage partiel**  \n",
        "   La phrase cible (franÃ§ais, dÃ©calÃ©e dâ€™un mot) passe dans GPT-2 :\n",
        "   $$\n",
        "   H_{dec} = \\text{Decoder}(y_{fr})\n",
        "   $$\n",
        "\n",
        "3. **Cross-Attention**  \n",
        "   Le dÃ©codeur reÃ§oit le contexte de lâ€™encodeur :\n",
        "   $$\n",
        "   \\text{context} = \\text{CrossAttn}(H_{dec}, H_{enc}, H_{enc})\n",
        "   $$\n",
        "\n",
        "4. **Fusion et prÃ©diction**  \n",
        "   On combine le contexte et la sortie du dÃ©codeur :\n",
        "   $$\n",
        "   H_{fused} = \\text{LayerNorm}(H_{dec} + \\text{context})\n",
        "   $$\n",
        "   puis on prÃ©dit le mot suivant :\n",
        "   $$\n",
        "   \\text{logits} = \\text{Linear}(H_{fused})\n",
        "   $$\n",
        "\n",
        "5. **Calcul de la perte (optionnel)**  \n",
        "   Si les Ã©tiquettes sont fournies :\n",
        "   $$\n",
        "   \\text{loss} = \\text{CrossEntropy}(\\text{logits}, \\text{labels})\n",
        "   $$\n",
        "\n"
      ],
      "metadata": {
        "id": "BpG9Eh2y0SVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderDecoderNMT(nn.Module):\n",
        "    def __init__(self, encoder, decoder_backbone, tok_fr, tie_weights=True, n_heads=12, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder_backbone\n",
        "        self.cross_attn = CrossAttention(d_model=self.decoder.config.n_embd,\n",
        "                                         n_heads=n_heads, dropout=dropout)\n",
        "        self.ln_fuse = nn.LayerNorm(self.decoder.config.n_embd)\n",
        "        self.lm_head = nn.Linear(self.decoder.config.n_embd, self.decoder.config.vocab_size, bias=False)\n",
        "        if tie_weights:\n",
        "            self.lm_head.weight = self.decoder.wte.weight\n",
        "\n",
        "    def forward(self, src_input_ids, src_attn_mask, tgt_input_ids, tgt_attn_mask=None, labels=None):\n",
        "        enc_out = self.encoder(input_ids=src_input_ids, attention_mask=src_attn_mask, return_dict=True)\n",
        "        H_enc = enc_out.last_hidden_state\n",
        "\n",
        "        dec_out = self.decoder(input_ids=tgt_input_ids, attention_mask=tgt_attn_mask,\n",
        "                               use_cache=False, return_dict=True)\n",
        "        H_dec = dec_out.last_hidden_state\n",
        "\n",
        "        context = self.cross_attn(H_dec, H_enc, H_enc, key_padding_mask=src_attn_mask)\n",
        "        H_fused = self.ln_fuse(H_dec + context)\n",
        "        logits = self.lm_head(H_fused)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)),\n",
        "                                   labels.view(-1),\n",
        "                                   ignore_index=-100)\n",
        "        return {\"logits\": logits, \"loss\": loss}\n"
      ],
      "metadata": {
        "id": "wpmeCT7nDk7X"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TranslationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df, tok_en, tok_fr, max_len=64):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.tok_en = tok_en\n",
        "        self.tok_fr = tok_fr\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        en_text = self.df.loc[idx, \"en\"]\n",
        "        fr_text = self.df.loc[idx, \"fr\"]\n",
        "\n",
        "        src = self.tok_en(en_text, truncation=True, padding=\"max_length\",\n",
        "                          max_length=self.max_len, return_tensors=\"pt\")\n",
        "        tgt = self.tok_fr(fr_text, truncation=True, padding=\"max_length\",\n",
        "                          max_length=self.max_len, return_tensors=\"pt\")\n",
        "\n",
        "        bos_id = self.tok_fr.bos_token_id or self.tok_fr.eos_token_id\n",
        "        pad_id = self.tok_fr.pad_token_id or self.tok_fr.eos_token_id\n",
        "\n",
        "        tgt_in = torch.cat([torch.tensor([[bos_id]]),\n",
        "                            tgt[\"input_ids\"][:, :-1]], dim=1)\n",
        "        labels = tgt[\"input_ids\"].clone()\n",
        "        labels[labels == pad_id] = -100\n",
        "\n",
        "        return {\n",
        "            \"src_input_ids\": src[\"input_ids\"].squeeze(0),\n",
        "            \"src_attn_mask\": src[\"attention_mask\"].squeeze(0),\n",
        "            \"tgt_input_ids\": tgt_in.squeeze(0),\n",
        "            \"tgt_attn_mask\": (tgt_in != pad_id).long().squeeze(0),\n",
        "            \"labels\": labels.squeeze(0),\n",
        "        }\n"
      ],
      "metadata": {
        "id": "1sMCOGJXDmLQ"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_data = TranslationDataset(train_df, tok_en, tok_fr, max_len=64)\n",
        "val_data = TranslationDataset(val_df, tok_en, tok_fr, max_len=64)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=8)\n"
      ],
      "metadata": {
        "id": "kS9shbMKDoCe"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = EncoderDecoderNMT(encoder, decoder_backbone, tok_fr).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\n"
      ],
      "metadata": {
        "id": "FXkZPd5DDpoa"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "scaler = GradScaler()\n",
        "device = \"cuda\"\n",
        "model.to(device)\n",
        "\n",
        "# Geler l'encodeur au dÃ©but\n",
        "for param in model.encoder.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Optimiseur\n",
        "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFzGTJp9EFiO",
        "outputId": "54500736-231d-4460-a650-f9524cf3512d"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1973884016.py:3: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()  # pour le mixed precision\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ” Boucle dâ€™entraÃ®nement\n",
        "\n",
        "Cette boucle entraÃ®ne le modÃ¨le sur plusieurs Ã©poques.  \n",
        "Pour chaque lot de donnÃ©es, elle calcule la **perte (loss)**, effectue la **rÃ©tropropagation** et met Ã  jour les poids du modÃ¨le.  \n",
        "Lâ€™utilisation de `autocast()` permet un entraÃ®nement plus rapide et plus lÃ©ger en **prÃ©cision mixte (FP16)** sur GPU.\n"
      ],
      "metadata": {
        "id": "hgDqaXQ32o1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(3):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(train_loader):\n",
        "        src_input_ids = batch[\"src_input_ids\"].to(device)\n",
        "        src_attn_mask = batch[\"src_attn_mask\"].to(device)\n",
        "        tgt_input_ids = batch[\"tgt_input_ids\"].to(device)\n",
        "        tgt_attn_mask = batch[\"tgt_attn_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # ðŸ”¹ FP16 automatique\n",
        "        with autocast():\n",
        "            out = model(src_input_ids, src_attn_mask, tgt_input_ids, tgt_attn_mask, labels)\n",
        "            loss = out[\"loss\"]\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(optimizer)\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Le4Oy8tLDrZz",
        "outputId": "7ea7a57b-bc3c-4b75-9cb5-28784883a7d1"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/19757 [00:00<?, ?it/s]/tmp/ipython-input-3434832626.py:14: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19757/19757 [43:56<00:00,  7.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Loss: 2.9251\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19757/19757 [43:46<00:00,  7.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 | Loss: 2.3738\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19757/19757 [43:40<00:00,  7.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 | Loss: 2.1341\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"bert2gpt2_translator.pt\")\n"
      ],
      "metadata": {
        "id": "sB1rMDtPidiN"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"bert2gpt2_translator.pt\", map_location=device))\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0YwLdI_iVgx",
        "outputId": "d69e02b3-e87a-482b-8b23-41a00192e3c5"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EncoderDecoderNMT(\n",
              "  (encoder): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (decoder): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D(nf=2304, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=768)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=3072, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=3072)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (cross_attn): CrossAttention(\n",
              "    (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (ln_fuse): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence(model, sentence_en, tok_en, tok_fr, device, max_len=40):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        src = tok_en(sentence_en, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "        enc_out = model.encoder(**src, return_dict=True).last_hidden_state\n",
        "\n",
        "        bos_id = tok_fr.bos_token_id or tok_fr.eos_token_id\n",
        "        generated = torch.tensor([[bos_id]], device=device)\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            out = model(src[\"input_ids\"], src[\"attention_mask\"], generated)\n",
        "            next_token = out[\"logits\"][:, -1, :].argmax(-1, keepdim=True)\n",
        "            generated = torch.cat([generated, next_token], dim=1)\n",
        "            if next_token.item() == tok_fr.eos_token_id:\n",
        "                break\n",
        "\n",
        "        return tok_fr.decode(generated[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "atn0Q-z8ipQ8"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Les rÃ©sultats montrent que le modÃ¨le traduit correctement des phrases simples et garde une bonne cohÃ©rence linguistique â€” un trÃ¨s bon signe que lâ€™architecture **BERTâ†’GPT2** apprend efficacement la correspondance entre lâ€™anglais et le franÃ§ais.\n"
      ],
      "metadata": {
        "id": "nk9_Tbbh4-tY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"I love you \"\n",
        "print(translate_sentence_simple(model, sentence, tok_en, tok_fr, device,max_len=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fxp3yHImjW7T",
        "outputId": "e1dd1a35-9088-4dae-a3af-a62d145c76dd"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Je vous aime.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"he is strong \"\n",
        "print(translate_sentence_simple(model, sentence, tok_en, tok_fr, device,max_len=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWdL-3HjkL8G",
        "outputId": "e5b73750-01e7-493c-cefe-e226ad32a4d4"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Il est fort.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence =\" I feel sick \"\n",
        "print(translate_sentence_simple(model, sentence, tok_en, tok_fr, device,max_len=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICHqVIzlkewQ",
        "outputId": "089853c4-161a-4697-e5aa-dea08ae3f4a0"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Je me sens mal\n"
          ]
        }
      ]
    }
  ]
}