{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##  Préparation du jeu de données anglais–français\n",
        "\n",
        "Nous utilisons le dataset public de Kaggle Language Translation (English–French) contenant deux colonnes : phrases anglaises et leurs équivalents français.  \n",
        "Les données sont chargées, renommées, nettoyées (valeurs manquantes, doublons, longueurs extrêmes) puis divisées en deux sous-ensembles :  \n",
        "-Train : 90 % des exemples pour l’entraînement du modèle  \n",
        "-Validation : 10 % pour l’évaluation des performances  \n",
        "\n",
        "Après nettoyage, on obtient environ 158 000 paires pour l’entraînement et 17 000 pour la validation.  \n",
        "Ce dataset servira à alimenter le modèle de traduction basé sur l’architecture BERT (encodeur) et GPT-2 (décodeur).\n"
      ],
      "metadata": {
        "id": "b1Pun9PTwXIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "RN27fD-_uRja"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(\"eng_-french.csv\")\n"
      ],
      "metadata": {
        "id": "VlzrECs7DcLd"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = df.rename(columns={\n",
        "    \"English words/sentences\": \"en\",\n",
        "    \"French words/sentences\": \"fr\"\n",
        "})\n",
        "\n",
        "df = df.dropna()\n",
        "df = df.drop_duplicates()\n",
        "df = df[(df[\"en\"].str.len() > 2) & (df[\"fr\"].str.len() > 2)]\n",
        "df = df[df[\"en\"].str.len() < 200]\n",
        "df = df[df[\"fr\"].str.len() < 200]\n",
        "\n",
        "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
        "print(f\"Train: {len(train_df)} | Val: {len(val_df)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNDBVVHZDb2m",
        "outputId": "05a97190-7539-45b8-f411-d2098b9448e2"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 158051 | Val: 17562\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Initialisation des modèles et tokenizers\n",
        "\n",
        "Dans cette partie, on importe les bibliothèques nécessaires (**Transformers**, **PyTorch**) et on charge deux modèles préentraînés :\n",
        "\n",
        "- **Encodeur : `bert-base-uncased`**  \n",
        "  Sert à comprendre les phrases anglaises et à produire leurs représentations vectorielles (embeddings contextuels).\n",
        "\n",
        "- **Décodeur : `dbddv01/gpt2-french-small`**  \n",
        "  Sert à générer les phrases traduites en français à partir des représentations fournies par l’encodeur.\n",
        "\n",
        "Les **tokenizers** correspondants sont également chargés pour convertir le texte brut en tokens numériques compatibles avec chaque modèle.  \n",
        "L’objectif est de créer une architecture **encoder–decoder hybride** :  \n",
        "BERT encode la phrase anglaise → GPT-2 décode et génère la traduction française.\n"
      ],
      "metadata": {
        "id": "noo0YBIGxNXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel, GPT2Model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n"
      ],
      "metadata": {
        "id": "afWi4gpYDX7Y"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_name = \"bert-base-uncased\"\n",
        "decoder_name = \"dbddv01/gpt2-french-small\"\n",
        "\n",
        "tok_en = AutoTokenizer.from_pretrained(encoder_name)\n",
        "tok_fr = AutoTokenizer.from_pretrained(decoder_name)\n",
        "\n",
        "encoder = AutoModel.from_pretrained(encoder_name)\n",
        "decoder_backbone = GPT2Model.from_pretrained(decoder_name)\n"
      ],
      "metadata": {
        "id": "b3WMQjboDZG0"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Détails mathématiques de la Cross-Attention\n",
        "\n",
        "La **cross-attention** est le mécanisme qui permet au **décodeur** (GPT-2) de \"regarder\" les représentations produites par l’**encodeur** (BERT).  \n",
        "Elle détermine quelles parties de la phrase source influencent la génération de chaque mot cible.\n",
        "\n",
        "---\n",
        "\n",
        "###  1. Les entrées\n",
        "\n",
        "On dispose de trois matrices :\n",
        "\n",
        "- $Q \\in \\mathbb{R}^{T_q \\times d_k}$ : les **requêtes** (*queries*) venant du décodeur  \n",
        "- $K \\in \\mathbb{R}^{T_k \\times d_k}$ : les **clés** (*keys*) venant de l’encodeur  \n",
        "- $V \\in \\mathbb{R}^{T_k \\times d_v}$ : les **valeurs** (*values*) venant aussi de l’encodeur  \n",
        "\n",
        "Chaque vecteur représente le sens d’un mot.  \n",
        "La cross-attention apprend à relier les mots de la phrase cible (français) à ceux de la phrase source (anglais).\n",
        "\n",
        "---\n",
        "\n",
        "###  2. Calcul des scores d’attention\n",
        "\n",
        "On mesure la similarité entre chaque requête et chaque clé :\n",
        "\n",
        "$$\n",
        "\\text{scores} = \\frac{QK^\\top}{\\sqrt{d_k}}\n",
        "$$\n",
        "\n",
        "- Le produit $QK^\\top$ produit une matrice de taille $T_q \\times T_k$, où chaque élément indique à quel point un mot cible “regarde” un mot source.  \n",
        "- Le facteur $\\frac{1}{\\sqrt{d_k}}$ évite que les valeurs soient trop grandes quand $d_k$ augmente (stabilisation numérique).\n",
        "\n",
        "---\n",
        "\n",
        "###  3. Application du softmax\n",
        "\n",
        "On transforme les scores en **poids de probabilité** :\n",
        "\n",
        "$$\n",
        "\\text{poids} = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)\n",
        "$$\n",
        "\n",
        "Chaque ligne de cette matrice correspond à une distribution de probabilité :  \n",
        "les poids indiquent l’importance relative de chaque mot anglais pour un mot français donné,  \n",
        "et la somme des poids d’une ligne vaut $1$.\n",
        "\n",
        "---\n",
        "\n",
        "###  4. Combinaison avec les valeurs\n",
        "\n",
        "Les poids sont utilisés pour combiner les représentations $V$ de l’encodeur :\n",
        "\n",
        "$$\n",
        "Z = \\text{poids} \\times V\n",
        "$$\n",
        "\n",
        "Chaque vecteur $Z_i$ est une **moyenne pondérée** des valeurs $V$,  \n",
        "où les poids déterminent quels mots sources contribuent le plus à la prédiction du mot cible.  \n",
        "\n",
        "Intuitivement :\n",
        "- le modèle \"pose une question\" via $Q$,  \n",
        "- et \"cherche la réponse\" dans la phrase source via $K$ et $V$.\n",
        "\n",
        "---\n",
        "\n",
        "###  5. Multi-head attention\n",
        "\n",
        "En pratique, on ne fait pas une seule attention, mais plusieurs têtes en parallèle :\n",
        "\n",
        "$$\n",
        "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O\n",
        "$$\n",
        "\n",
        "Chaque tête apprend une manière différente de relier les mots  \n",
        "(structure grammaticale, dépendances, contexte, etc.).  \n",
        "Leur combinaison donne une vision plus complète des relations entre mots anglais et français.\n",
        "\n",
        "---\n",
        "\n",
        "###  En résumé\n",
        "\n",
        "1. L’encodeur (BERT) produit des représentations $K$ et $V$ de la phrase anglaise.  \n",
        "2. Le décodeur (GPT-2) génère ses requêtes $Q$ pour chaque mot français.  \n",
        "3. La cross-attention calcule comment chaque $Q_i$ doit combiner les $V_j$ de la phrase source.  \n",
        "4. Le résultat $Z$ guide la génération du mot suivant.\n",
        "\n",
        "Ainsi, la couche de **cross-attention** fait le lien mathématique entre  \n",
        "**la compréhension (encodeur)** et **la génération (décodeur)**.\n"
      ],
      "metadata": {
        "id": "ET1G9WILyZ9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, d_model=768, n_heads=12, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.q_proj = nn.Linear(d_model, d_model)\n",
        "        self.k_proj = nn.Linear(d_model, d_model)\n",
        "        self.v_proj = nn.Linear(d_model, d_model)\n",
        "        self.out_proj = nn.Linear(d_model, d_model)\n",
        "        self.n_heads = n_heads\n",
        "        self.d_head = d_model // n_heads\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def _split_heads(self, x):\n",
        "        B, T, D = x.shape\n",
        "        return x.view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
        "\n",
        "    def _merge_heads(self, x):\n",
        "        B, h, T, d = x.shape\n",
        "        return x.transpose(1, 2).contiguous().view(B, T, h * d)\n",
        "\n",
        "    def forward(self, q, k, v, key_padding_mask=None):\n",
        "        Q = self._split_heads(self.q_proj(q))\n",
        "        K = self._split_heads(self.k_proj(k))\n",
        "        V = self._split_heads(self.v_proj(v))\n",
        "\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_head ** 0.5)\n",
        "        if key_padding_mask is not None:\n",
        "            mask = key_padding_mask.unsqueeze(1).unsqueeze(2)\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "        context = torch.matmul(attn, V)\n",
        "        context = self._merge_heads(context)\n",
        "        return self.out_proj(context)\n"
      ],
      "metadata": {
        "id": "Q7ybRDuEDjld"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classe `EncoderDecoderNMT` — Architecture complète du traducteur\n",
        "\n",
        "Cette classe assemble toutes les composantes du modèle de traduction :  \n",
        "un **encodeur BERT**, un **décodeur GPT-2**, et une **couche de cross-attention** qui relie les deux.\n",
        "\n",
        "\n",
        "### Fonctionnement du `forward`\n",
        "\n",
        "1. **Encodage**  \n",
        "   La phrase source (anglais) passe dans BERT :\n",
        "   $$\n",
        "   H_{enc} = \\text{Encoder}(x_{en})\n",
        "   $$\n",
        "\n",
        "2. **Décodage partiel**  \n",
        "   La phrase cible (français, décalée d’un mot) passe dans GPT-2 :\n",
        "   $$\n",
        "   H_{dec} = \\text{Decoder}(y_{fr})\n",
        "   $$\n",
        "\n",
        "3. **Cross-Attention**  \n",
        "   Le décodeur reçoit le contexte de l’encodeur :\n",
        "   $$\n",
        "   \\text{context} = \\text{CrossAttn}(H_{dec}, H_{enc}, H_{enc})\n",
        "   $$\n",
        "\n",
        "4. **Fusion et prédiction**  \n",
        "   On combine le contexte et la sortie du décodeur :\n",
        "   $$\n",
        "   H_{fused} = \\text{LayerNorm}(H_{dec} + \\text{context})\n",
        "   $$\n",
        "   puis on prédit le mot suivant :\n",
        "   $$\n",
        "   \\text{logits} = \\text{Linear}(H_{fused})\n",
        "   $$\n",
        "\n",
        "5. **Calcul de la perte (optionnel)**  \n",
        "   Si les étiquettes sont fournies :\n",
        "   $$\n",
        "   \\text{loss} = \\text{CrossEntropy}(\\text{logits}, \\text{labels})\n",
        "   $$\n",
        "\n"
      ],
      "metadata": {
        "id": "BpG9Eh2y0SVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderDecoderNMT(nn.Module):\n",
        "    def __init__(self, encoder, decoder_backbone, tok_fr, tie_weights=True, n_heads=12, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder_backbone\n",
        "        self.cross_attn = CrossAttention(d_model=self.decoder.config.n_embd,\n",
        "                                         n_heads=n_heads, dropout=dropout)\n",
        "        self.ln_fuse = nn.LayerNorm(self.decoder.config.n_embd)\n",
        "        self.lm_head = nn.Linear(self.decoder.config.n_embd, self.decoder.config.vocab_size, bias=False)\n",
        "        if tie_weights:\n",
        "            self.lm_head.weight = self.decoder.wte.weight\n",
        "\n",
        "    def forward(self, src_input_ids, src_attn_mask, tgt_input_ids, tgt_attn_mask=None, labels=None):\n",
        "        enc_out = self.encoder(input_ids=src_input_ids, attention_mask=src_attn_mask, return_dict=True)\n",
        "        H_enc = enc_out.last_hidden_state\n",
        "\n",
        "        dec_out = self.decoder(input_ids=tgt_input_ids, attention_mask=tgt_attn_mask,\n",
        "                               use_cache=False, return_dict=True)\n",
        "        H_dec = dec_out.last_hidden_state\n",
        "\n",
        "        context = self.cross_attn(H_dec, H_enc, H_enc, key_padding_mask=src_attn_mask)\n",
        "        H_fused = self.ln_fuse(H_dec + context)\n",
        "        logits = self.lm_head(H_fused)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)),\n",
        "                                   labels.view(-1),\n",
        "                                   ignore_index=-100)\n",
        "        return {\"logits\": logits, \"loss\": loss}\n"
      ],
      "metadata": {
        "id": "wpmeCT7nDk7X"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TranslationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df, tok_en, tok_fr, max_len=64):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.tok_en = tok_en\n",
        "        self.tok_fr = tok_fr\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        en_text = self.df.loc[idx, \"en\"]\n",
        "        fr_text = self.df.loc[idx, \"fr\"]\n",
        "\n",
        "        src = self.tok_en(en_text, truncation=True, padding=\"max_length\",\n",
        "                          max_length=self.max_len, return_tensors=\"pt\")\n",
        "        tgt = self.tok_fr(fr_text, truncation=True, padding=\"max_length\",\n",
        "                          max_length=self.max_len, return_tensors=\"pt\")\n",
        "\n",
        "        bos_id = self.tok_fr.bos_token_id or self.tok_fr.eos_token_id\n",
        "        pad_id = self.tok_fr.pad_token_id or self.tok_fr.eos_token_id\n",
        "\n",
        "        tgt_in = torch.cat([torch.tensor([[bos_id]]),\n",
        "                            tgt[\"input_ids\"][:, :-1]], dim=1)\n",
        "        labels = tgt[\"input_ids\"].clone()\n",
        "        labels[labels == pad_id] = -100\n",
        "\n",
        "        return {\n",
        "            \"src_input_ids\": src[\"input_ids\"].squeeze(0),\n",
        "            \"src_attn_mask\": src[\"attention_mask\"].squeeze(0),\n",
        "            \"tgt_input_ids\": tgt_in.squeeze(0),\n",
        "            \"tgt_attn_mask\": (tgt_in != pad_id).long().squeeze(0),\n",
        "            \"labels\": labels.squeeze(0),\n",
        "        }\n"
      ],
      "metadata": {
        "id": "1sMCOGJXDmLQ"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_data = TranslationDataset(train_df, tok_en, tok_fr, max_len=64)\n",
        "val_data = TranslationDataset(val_df, tok_en, tok_fr, max_len=64)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=8)\n"
      ],
      "metadata": {
        "id": "kS9shbMKDoCe"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = EncoderDecoderNMT(encoder, decoder_backbone, tok_fr).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\n"
      ],
      "metadata": {
        "id": "FXkZPd5DDpoa"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "scaler = GradScaler()\n",
        "device = \"cuda\"\n",
        "model.to(device)\n",
        "\n",
        "# Geler l'encodeur au début\n",
        "for param in model.encoder.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Optimiseur\n",
        "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFzGTJp9EFiO",
        "outputId": "54500736-231d-4460-a650-f9524cf3512d"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1973884016.py:3: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()  # pour le mixed precision\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🔁 Boucle d’entraînement\n",
        "\n",
        "Cette boucle entraîne le modèle sur plusieurs époques.  \n",
        "Pour chaque lot de données, elle calcule la **perte (loss)**, effectue la **rétropropagation** et met à jour les poids du modèle.  \n",
        "L’utilisation de `autocast()` permet un entraînement plus rapide et plus léger en **précision mixte (FP16)** sur GPU.\n"
      ],
      "metadata": {
        "id": "hgDqaXQ32o1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(3):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(train_loader):\n",
        "        src_input_ids = batch[\"src_input_ids\"].to(device)\n",
        "        src_attn_mask = batch[\"src_attn_mask\"].to(device)\n",
        "        tgt_input_ids = batch[\"tgt_input_ids\"].to(device)\n",
        "        tgt_attn_mask = batch[\"tgt_attn_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 🔹 FP16 automatique\n",
        "        with autocast():\n",
        "            out = model(src_input_ids, src_attn_mask, tgt_input_ids, tgt_attn_mask, labels)\n",
        "            loss = out[\"loss\"]\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(optimizer)\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Le4Oy8tLDrZz",
        "outputId": "7ea7a57b-bc3c-4b75-9cb5-28784883a7d1"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/19757 [00:00<?, ?it/s]/tmp/ipython-input-3434832626.py:14: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "100%|██████████| 19757/19757 [43:56<00:00,  7.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Loss: 2.9251\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19757/19757 [43:46<00:00,  7.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 | Loss: 2.3738\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19757/19757 [43:40<00:00,  7.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 | Loss: 2.1341\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"bert2gpt2_translator.pt\")\n"
      ],
      "metadata": {
        "id": "sB1rMDtPidiN"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"bert2gpt2_translator.pt\", map_location=device))\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0YwLdI_iVgx",
        "outputId": "d69e02b3-e87a-482b-8b23-41a00192e3c5"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EncoderDecoderNMT(\n",
              "  (encoder): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (decoder): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D(nf=2304, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=768)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=3072, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=3072)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (cross_attn): CrossAttention(\n",
              "    (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (ln_fuse): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence(model, sentence_en, tok_en, tok_fr, device, max_len=40):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        src = tok_en(sentence_en, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "        enc_out = model.encoder(**src, return_dict=True).last_hidden_state\n",
        "\n",
        "        bos_id = tok_fr.bos_token_id or tok_fr.eos_token_id\n",
        "        generated = torch.tensor([[bos_id]], device=device)\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            out = model(src[\"input_ids\"], src[\"attention_mask\"], generated)\n",
        "            next_token = out[\"logits\"][:, -1, :].argmax(-1, keepdim=True)\n",
        "            generated = torch.cat([generated, next_token], dim=1)\n",
        "            if next_token.item() == tok_fr.eos_token_id:\n",
        "                break\n",
        "\n",
        "        return tok_fr.decode(generated[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "atn0Q-z8ipQ8"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Les résultats montrent que le modèle traduit correctement des phrases simples et garde une bonne cohérence linguistique — un très bon signe que l’architecture **BERT→GPT2** apprend efficacement la correspondance entre l’anglais et le français.\n"
      ],
      "metadata": {
        "id": "nk9_Tbbh4-tY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"I love you \"\n",
        "print(translate_sentence_simple(model, sentence, tok_en, tok_fr, device,max_len=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fxp3yHImjW7T",
        "outputId": "e1dd1a35-9088-4dae-a3af-a62d145c76dd"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Je vous aime.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"he is strong \"\n",
        "print(translate_sentence_simple(model, sentence, tok_en, tok_fr, device,max_len=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWdL-3HjkL8G",
        "outputId": "e5b73750-01e7-493c-cefe-e226ad32a4d4"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Il est fort.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence =\" I feel sick \"\n",
        "print(translate_sentence_simple(model, sentence, tok_en, tok_fr, device,max_len=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICHqVIzlkewQ",
        "outputId": "089853c4-161a-4697-e5aa-dea08ae3f4a0"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Je me sens mal\n"
          ]
        }
      ]
    }
  ]
}